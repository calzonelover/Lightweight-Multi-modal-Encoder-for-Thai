{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c45d79-f4ce-4dd2-8fad-0ce3e0787774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAPTION_FIELD = \"sentences_raw\"\n",
    "CAPTION_FIELD = \"th_sentences_raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3e8f30-ea7d-418a-bbb6-e4a8dffc0194",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "# os.environ[\"HF_DATASETS_CACHE\"] = \"/workspace/cache\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3cf748-966f-43bb-a7dc-4115d712f5db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"patomp/thai-mscoco-2014-captions\", split=\"test\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121366ef-3da6-4ad8-ab8b-70563ea11414",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fd2c43-df88-4e10-a357-de4efd7eb04f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = dataset[\"image\"]\n",
    "texts = [captions[0] for captions in dataset[CAPTION_FIELD]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ecbb7e-c9db-4a3a-89a0-e1b89b3302b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### ======== Multi-lingual Model ========\n",
    "### (1) [Sentence-Transformers] Multilingual\n",
    "# TEXT_ENCODER_MODEL_NAME = \"sentence-transformers/clip-ViT-B-32-multilingual-v1\"\n",
    "# img_model = SentenceTransformer('clip-ViT-B-32')\n",
    "# text_model = SentenceTransformer(TEXT_ENCODER_MODEL_NAME)\n",
    "# image_embeddings = img_model.encode(images)\n",
    "# stime = time.time()\n",
    "# text_embeddings = text_model.encode(texts)\n",
    "# etime = time.time()\n",
    "\n",
    "### (2) [Sentence-Transformers] Multilingual\n",
    "# import clip\n",
    "# from multilingual_clip import pt_multilingual_clip\n",
    "# import transformers\n",
    "\n",
    "# model_name = 'M-CLIP/XLM-Roberta-Large-Vit-B-32'\n",
    "# text_model = pt_multilingual_clip.MultilingualCLIP.from_pretrained(model_name)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# visual_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     stime = time.time()\n",
    "#     text_embeddings = [text_model.forward([txt,], tokenizer)[0] for txt in texts]\n",
    "#     etime = time.time()\n",
    "#     image_embeddings = [visual_model.encode_image(preprocess(img).unsqueeze(0).to(device))[0] for img in images]\n",
    "\n",
    "# text_embeddings = np.array([x.detach().numpy() for x in text_embeddings])\n",
    "# image_embeddings = np.array([x.detach().numpy() for x in image_embeddings])\n",
    "\n",
    "\n",
    "### ======== Thai-only Model ========\n",
    "### (1) Thai-Cross-CLIP\n",
    "# import sys\n",
    "# sys.path.append(\"./Thai-Cross-CLIP\")\n",
    "# from source.model import *\n",
    "# from source.config import *\n",
    "# sys.path.append(\"./thai2transformers/thai2transformers\")\n",
    "# from preprocess import process_transformers\n",
    "# import clip\n",
    "# text_model = TextModel().to(CFG.device)\n",
    "# text_model.load_state_dict(torch.load(\"./CLIP-MSE-WangchanBerta/text_MSE_2m.pt\", map_location=CFG.device))\n",
    "# text_model.eval().requires_grad_(False)\n",
    "# clip_model, compose = clip.load('ViT-B/32')\n",
    "# clip_model.to(CFG.device).eval()\n",
    "# input_resolution = clip_model.visual.input_resolution\n",
    "# print(\"Text encoder parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in text_model.parameters()]):,}\")\n",
    "# print(\"Input image resolution:\", input_resolution)\n",
    "# _images = [img.convert('RGB').resize((input_resolution,input_resolution)) for img in images]\n",
    "# _images = [torch.tensor(np.array(img)).permute(2, 0, 1)/255 for img in _images]\n",
    "# _images = [img.unsqueeze(0).to(CFG.device) for img in _images]\n",
    "# with torch.no_grad():\n",
    "#     image_embeddings = np.array([clip_model.encode_image(img).detach().numpy() for img in _images])\n",
    "#     stime = time.time()\n",
    "#     text_embeddings = [text_model.encode_text([process_transformers(txt)]) for txt in texts]\n",
    "#     etime = time.time()\n",
    "#     text_embeddings= [txt.to(CFG.device).detach().numpy() for txt in text_embeddings]\n",
    "#     text_embeddings = np.array(text_embeddings)\n",
    "# image_embeddings = image_embeddings.reshape(-1, 512)\n",
    "# text_embeddings = text_embeddings.reshape(-1, 512)\n",
    "\n",
    "### (2) Thai2Fit\n",
    "# import sys\n",
    "# sys.path.append(\"../models\")\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from pythainlp import word_vector, word_tokenize\n",
    "# from projector import Projector\n",
    "# projector = Projector(input_embedding_dim=300)\n",
    "# projector.load_state_dict(torch.load(\"../models/projector_high_alpha.pt\"))\n",
    "# projector.eval()\n",
    "# model = word_vector.WordVector(model_name=\"thai2fit_wv\")#.get_model()\n",
    "# def embed_sentence(text):\n",
    "#     embed = model.sentence_vectorizer(text, use_mean=True)[0]\n",
    "#     return projector(torch.from_numpy(embed).float())\n",
    "# with torch.no_grad():\n",
    "#     stime = time.time()\n",
    "#     text_embeddings = [embed_sentence(x) for x in texts]\n",
    "#     etime = time.time()\n",
    "# text_embeddings = np.array([x.detach().numpy() for x in text_embeddings])\n",
    "\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"patomp/thai-light-multimodal-clip-and-distill\", trust_remote_code=True)\n",
    "text_embeddings = np.array([model(text) for text in texts])\n",
    "\n",
    "import clip\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "visual_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "with torch.no_grad():\n",
    "    image_embeddings = [visual_model.encode_image(preprocess(img).unsqueeze(0).to(device))[0] for img in images]\n",
    "image_embeddings = np.array([x.detach().numpy() for x in image_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e1bcd-daf0-4d6c-9621-4b7f15959719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latent_time = etime - stime\n",
    "sample_per_sec = float(len(texts)) / latent_time\n",
    "\n",
    "print(\"latent_time: \", latent_time)\n",
    "print(\"sample_per_sec: \", sample_per_sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d813b186-b69a-4f6e-a62f-cf5f879d4f51",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c50341-4ce4-4392-82cb-e7d75fa670c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference for FAISS Index: https://github.com/facebookresearch/faiss/wiki/Faiss-indexes\n",
    "import faiss\n",
    "\n",
    "d = text_embeddings.shape[1]\n",
    "assert d == image_embeddings.shape[1]\n",
    "\n",
    "text_index = faiss.IndexFlatIP(d)\n",
    "image_index = faiss.IndexFlatIP(d)\n",
    "\n",
    "assert text_index.ntotal == image_index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f664c223-19fc-44d4-ae84-53dc816e6993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_embeddings = image_embeddings / np.linalg.norm(image_embeddings, axis=1).reshape(-1, 1)\n",
    "text_embeddings = text_embeddings / np.linalg.norm(text_embeddings, axis=1).reshape(-1, 1)\n",
    "\n",
    "image_index.add(image_embeddings)\n",
    "text_index.add(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d1b53b-9589-4672-9972-c31b82028294",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# np.linalg.norm(text_embeddings, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713bf15a-33c9-4510-87c7-bea0b95c2b45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(d):\n",
    "    sample_embd = text_embeddings[d]\n",
    "    cosim_score = np.inner(sample_embd, sample_embd)\n",
    "    assert cosim_score > 0.99 and cosim_score < 1.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfac849d-b477-42ff-ba63-934b978db2dc",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e857a0-a21c-4334-8a2c-6264ca894db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall_at_k(a_modal_embeddings, b_modal_index, k=5) -> float:    \n",
    "    _, _retrieved_indices = b_modal_index.search(a_modal_embeddings, k=k)\n",
    "    # print(_retrieved_indices)\n",
    "    _n = len(a_modal_embeddings)\n",
    "    _recall = [\n",
    "        1.0 if i in indices else 0.0\n",
    "        for i,indices in zip(range(_n),_retrieved_indices)\n",
    "    ]\n",
    "    _recall = sum(_recall) / float(_n)\n",
    "    return _recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49416078-a288-48a4-90ea-71171c6a6188",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# expect > .99\n",
    "get_recall_at_k(text_embeddings, text_index, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fccbdd-78af-4719-8668-a41197b59fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# expect > .99\n",
    "get_recall_at_k(image_embeddings, image_index, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009ef46a-5657-426b-9537-c8904ddc1df7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_recall_at_k(text_embeddings, image_index,k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74716f91-50a0-4b88-9c5e-7646488bfb6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_recall_at_k(text_embeddings, image_index,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423414c9-f3c6-466d-84ad-d49e359abbb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_recall_at_k(image_embeddings, text_index,k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c3938-36cf-419e-82d3-f49170f54985",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_recall_at_k(image_embeddings, text_index,k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482efa37-9048-4830-9fef-ade29fc24a4c",
   "metadata": {},
   "source": [
    "## Query Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc3a53-bd2e-45e5-9d5a-21e9ea7d79f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"หมากำลังวิ่งเล่น\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b417e-dc89-4151-bf5b-8b8b769ab9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code came from sbert multilingual\n",
    "embd = text_model.encode([text,])\n",
    "_, indices = image_index.search(embd, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2201e446-9a4e-4814-8140-1e7b474dc747",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images[indices[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3303b7f6-27c8-4f34-9498-2187fa8186cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images[indices[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1ad534-9da1-47d2-8753-a42ff85033cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images[indices[0][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea335c7b-de6d-402a-8769-468c4e0b682a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
